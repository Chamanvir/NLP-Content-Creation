{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Python implementation of CBOW and Skip-gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, I will show you how to implement word2vec using the standard Python library, NumPy and two utility functions from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  STEP 1: Requirements:\n",
    "    - Python version: 3.6\n",
    "    - Numpy version: 1.14.2\n",
    "    - keras version: 2.1.5 [It is used because of a few useful NLP tools (Tokenizer, sequence and np_utils)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: From corpus to center and context words\n",
    "    - The first step in our implementation is to transform a text corpus into numbers. Specifically, into one-hot encoded vectors. Recall that in word2vec we scan through a text corpus and for each training example we define a center word with its surrounding context words. Depending on the algorithm of choice (Continuous Bag-of-Words or Skip-gram), the center and context words may work as inputs and labels, respectively, or vice versa.\n",
    "    - Typically the context words are defined as a symmetric window of predefined length, on both the left and right hand sides of the center word. For example, suppose our corpus consists of the sentence “I like playing football with my friends”. Also, let’s say that we define our window to be symmetric around the center word and of length two. Then, our one-hot encoded context and center words can be visualized as follows,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first we need to tokenize the corpus text to accomplish this mapping, from text to one-hot-encoded vectors, as displayed in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the corpus text:\n",
    "    - param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    - return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\n",
    "    - return V: size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, mapping from text to one-hot encoded context and center words using the function corpus2io which uses the auxiliary function to_categorical (copied from the Keras repository).\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "        - Arguments\n",
    "            - y: class vector to be converted into a matrix(integers from 0 to num_classes).\n",
    "            - num_classes: total number of classes.\n",
    "        - Returns\n",
    "            - A binary matrix representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus2io:\n",
    "    - Converts corpus text into context and center words\n",
    "        - Arguments\n",
    "            - corpus_tokenized: corpus text\n",
    "            - window_size: size of context window\n",
    "        - Returns\n",
    "            - context and center words (arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            center = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts = contexts + [words[i]-1 for i in range(s, e) if 0 <= i < L and i != index]\n",
    "            center.append(word-1)\n",
    "            # x has shape c x V where c is size of contexts\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            # y has shape k x V where k is number of center words\n",
    "            y = np_utils.to_categorical(center, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " center word = [[1. 0. 0. 0. 0. 0. 0.]] \n",
      " context words =\n",
      " [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "1 \n",
      " center word = [[0. 1. 0. 0. 0. 0. 0.]] \n",
      " context words =\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "2 \n",
      " center word = [[0. 0. 1. 0. 0. 0. 0.]] \n",
      " context words =\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "3 \n",
      " center word = [[0. 0. 0. 1. 0. 0. 0.]] \n",
      " context words =\n",
      " [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "4 \n",
      " center word = [[0. 0. 0. 0. 1. 0. 0.]] \n",
      " context words =\n",
      " [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "5 \n",
      " center word = [[0. 0. 0. 0. 0. 1. 0.]] \n",
      " context words =\n",
      " [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "6 \n",
      " center word = [[0. 0. 0. 0. 0. 0. 1.]] \n",
      " context words =\n",
      " [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "corpus = [\"I like playing football with my friends\"]\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "for i, (x, y) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    print(i, \"\\n center word =\", y, \"\\n context words =\\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  STEP 3: Softmax function \n",
    "    It calculates softmax based probability for given input vector\n",
    "        - Arguments\n",
    "            - x: numpy array/list\n",
    "        - Returns\n",
    "            - softmax of input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Python code for the Multi-Word CBOW model: \n",
    "    Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "        - param context: all the context words (these represent the inputs)\n",
    "        - param label: the center word (this represents the label)\n",
    "        - param W1: weights from the input to the hidden layer\n",
    "        - param W2: weights from the hidden to the output layer\n",
    "        - param loss: float that represents the current value of the loss function\n",
    "        - return: updated weights and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cbow(context, label, W1, W2, loss):\n",
    "        # context is 'x' from tokenizer, it is a c x V matrix\n",
    "        # label is 'y' from tokenizer, it is a 1 x V matrix\n",
    "        \n",
    "        x = np.matrix(np.mean(context, axis=0))\n",
    "        \n",
    "        # x is a 1 x V matrix\n",
    "        # W1 is a VxN matrix\n",
    "        # h is a N x 1 matrix\n",
    "        h = np.matmul(W1.T, x.T)\n",
    "        \n",
    "        # u is a V x 1 matrix\n",
    "        u = np.matmul(W2.T, h)\n",
    "        \n",
    "        # W2 is an N x V matrix\n",
    "        # y_pred is a V x 1 matrix\n",
    "        y_pred = softmax(u)\n",
    "        # e is a V x 1 matrix\n",
    "        e = -label.T + y_pred\n",
    "        # h is N x 1 and e is V x 1 so dW2 is N x V\n",
    "        dW2 = np.outer(h, e)\n",
    "        # x.T is a V x 1 matrix, W2e is a Nx1 so dW1 this is V x N\n",
    "        dW1 = np.outer(x.T, np.matmul(W2, e))\n",
    "\n",
    "        new_W1 = W1 - eta * dW1\n",
    "        new_W2 = W2 - eta * dW2\n",
    "\n",
    "        # label is a 1xV matrix so label.T is a Vx1 matrix\n",
    "        loss += -float(u[label.T == 1]) + np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of above function is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example #0 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[1. 0. 0. 0. 0. 0. 0.]], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.54340494  0.27836939]\n",
      " [ 0.40739238  0.86832668]\n",
      " [-0.01240636  0.14511967]\n",
      " [ 0.67074908  0.82585276]\n",
      " [ 0.13670659  0.57509333]\n",
      " [ 0.89132195  0.20920212]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[2.37522044e-01 9.74588882e-01 8.08598345e-01 1.69452859e-01\n",
      "  8.13081663e-01 2.71730692e-01 4.28973634e-01]\n",
      " [9.80158449e-01 8.08565553e-01 3.29167094e-01 1.69808842e-01\n",
      "  3.65755980e-01 4.13558689e-04 2.46279033e-01]] \n",
      "\t loss = 1.7750419435896188\n",
      "\n",
      "Training example #1 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 1. 0. 0. 0. 0. 0.]], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "\t W1 = [[5.56639604e-01 2.89686158e-01]\n",
      " [4.07392378e-01 8.68326677e-01]\n",
      " [8.28305455e-04 1.56436438e-01]\n",
      " [6.83983747e-01 8.37169528e-01]\n",
      " [1.36706590e-01 5.75093329e-01]\n",
      " [8.91321954e-01 2.09202122e-01]\n",
      " [1.85328220e-01 1.08376890e-01]]\n",
      "\t W2 = [[ 0.23121195  1.00675437  0.80254985  0.16507125  0.80692926  0.26747677\n",
      "   0.42395468]\n",
      " [ 0.97359845  0.84200492  0.32287905  0.16525371  0.35935991 -0.00400884\n",
      "   0.24106131]] \n",
      "\t loss = 3.3994293405031777\n",
      "\n",
      "Training example #2 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 1. 0. 0. 0. 0.]], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "\t W1 = [[5.61872952e-01 2.85249823e-01]\n",
      " [4.12625726e-01 8.63890342e-01]\n",
      " [8.28305455e-04 1.56436438e-01]\n",
      " [6.89217095e-01 8.32733193e-01]\n",
      " [1.41939938e-01 5.70656995e-01]\n",
      " [8.91321954e-01 2.09202122e-01]\n",
      " [1.85328220e-01 1.08376890e-01]]\n",
      "\t W2 = [[ 0.22354561  0.99679699  0.84065603  0.16064342  0.80024989  0.26331958\n",
      "   0.41873659]\n",
      " [ 0.96255774  0.82766477  0.37775784  0.15887694  0.3497406  -0.00999584\n",
      "   0.23354646]] \n",
      "\t loss = 5.3239411874814575\n",
      "\n",
      "Training example #3 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 1. 0. 0. 0.]], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00966035  0.14854727]\n",
      " [ 0.68921709  0.83273319]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88083329  0.20131295]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[ 0.21775807  0.98959276  0.83509537  0.19287099  0.79483857  0.25952945\n",
      "   0.4142629 ]\n",
      " [ 0.95535615  0.81870036  0.37083857  0.19897854  0.34300715 -0.01471198\n",
      "   0.22797972]] \n",
      "\t loss = 7.540849578690909\n",
      "\n",
      "Training example #4 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 0. 1. 0. 0.]], \n",
      " \t context = [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.69455676  0.82980761]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.19066788  0.1054513 ]]\n",
      "\t W2 = [[ 0.21139714  0.98106847  0.82819932  0.18794169  0.83176617  0.25479284\n",
      "   0.40878249]\n",
      " [ 0.95065221  0.81239659  0.36573889  0.19533328  0.37031536 -0.01821474\n",
      "   0.22392692]] \n",
      "\t loss = 9.412493726530963\n",
      "\n",
      "Training example #5 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 0. 0. 1. 0.]], \n",
      " \t context = [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.12071664  0.54631869]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.2058461  0.97434378 0.82309063 0.18416489 0.82663959 0.28520946\n",
      "  0.40465366]\n",
      " [0.94247299 0.80248806 0.35821146 0.18976835 0.36276158 0.0266028\n",
      "  0.21784328]] \n",
      "\t loss = 11.690693352573714\n",
      "\n",
      "Training example #6 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 0. 0. 0. 1.]], \n",
      " \t context = [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.19857939 0.96418767 0.81511412 0.17873393 0.81863527 0.27983192\n",
      "  0.44886582]\n",
      " [0.93709845 0.7949765  0.35231195 0.18575155 0.3568415  0.02262551\n",
      "  0.25054305]] \n",
      "\t loss = 13.796008268936973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#user-defined parameters\n",
    "corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "window_size = 2 #symmetrical\n",
    "eta = 0.1 #learning rate\n",
    "\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "\n",
    "#initialize weights (with random values) and loss function\n",
    "np.random.seed(100)\n",
    "W1 = np.random.rand(V, N)\n",
    "W2 = np.random.rand(N, V)\n",
    "loss = 0.\n",
    "\n",
    "for i, (context, label) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    W1, W2, loss = cbow(context, label, W1, W2, loss)\n",
    "    print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t context = {}\".format(i, label, context))\n",
    "    print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  STEP 5: Python code for the Skip-Gram model: \n",
    "    In the skip-gram model, the inputs are represented by center words and the labels by context words. The full set of equations that we need to solve are the following:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The update equation for the weights is the same as for the CBOW model,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Python code that solves the equations for the skip-gram model:\n",
    "    - param context: all the context words (these represent the labels)\n",
    "    - param x: the center word (this represents the input)\n",
    "    - param W1: weights from the input to the hidden layer\n",
    "    - param W2: weights from the hidden to the output layer\n",
    "    - param loss: float that represents the current value of the loss function\n",
    "    - return: updated weights and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(context, x, W1, W2, loss):\n",
    "        # context is \"x\" from tokenizer, it is a c x V matrix\n",
    "        # \"x\" is \"y\" from tokenizer, it is a 1 x V matrix\n",
    "        # W1 has dimension V x N (N= number of features, V = vocab size)\n",
    "        # x has dimension V x 1\n",
    "        h = np.matmul(W1.T, x.T)\n",
    "        # h has dimension N x 1\n",
    "        # W2 has dimension N x V\n",
    "        # u has dimension V x 1\n",
    "        u = np.dot(W2.T, h)\n",
    "        # y_pred has dimension V x 1\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        # context is a c by V matrix\n",
    "        # e is a V x c matrix\n",
    "        e = np.outer(y_pred,np.array([1]*context.shape[0]))-context.T\n",
    "\n",
    "        # np.sum(e, axis=1) is a V x 1 vectors\n",
    "        # h is an N x 1 Vector\n",
    "        # dW2 is a N x V matrix\n",
    "        dW2 = np.outer(h, np.sum(e, axis=1))\n",
    "        # x is a V x 1 matrix\n",
    "        # np.dot(W2, np.sum(e,axis=1)) is a product (N x V) (Vx 1) is Nx1\n",
    "        # dW1 is an V x N matrix\n",
    "        dW1 = np.outer(x, np.dot(W2, np.sum(e, axis=1)))\n",
    "\n",
    "        new_W1 = W1 - eta * dW1\n",
    "        new_W2 = W2 - eta * dW2\n",
    "\n",
    "        loss += - np.sum([u[label.T == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function skipgram defined above is used similarly to the cbow function, except that now the center words are no more the “labels” but are actually the inputs and the labels are represented by the context words. Using the same user-defined parameters as for the CBOW case, we can run the same example with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example #0 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]], \n",
      " \t center = [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.18344406 0.99802854 0.85318831 0.16665383 0.80046283 0.26762708\n",
      "  0.43454347]\n",
      " [0.9294146  0.81215672 0.37164133 0.17961877 0.34761577 0.0164294\n",
      "  0.24327192]] \n",
      "\t loss = 17.236381960861195\n",
      "\n",
      "Training example #1 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]], \n",
      " \t center = [[0. 1. 0. 0. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[ 0.20155094  0.97028265  0.87544896  0.19531034  0.7832442   0.25716013\n",
      "   0.42095089]\n",
      " [ 0.96795747  0.75309597  0.41902602  0.24061789  0.31096368 -0.00585087\n",
      "   0.21433836]] \n",
      "\t loss = 23.183887767745013\n",
      "\n",
      "Training example #2 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]], \n",
      " \t center = [[0. 0. 1. 0. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.0018399   0.20147359]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[ 0.20138661  0.9701092   0.8756954   0.1951191   0.78305483  0.25739241\n",
      "   0.42119057]\n",
      " [ 0.97349621  0.75894167  0.41072004  0.24706327  0.31734629 -0.01367941\n",
      "   0.20626045]] \n",
      "\t loss = 30.88216196281432\n",
      "\n",
      "Training example #3 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]], \n",
      " \t center = [[0. 0. 0. 1. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.0018399   0.20147359]\n",
      " [ 0.72776396  0.75861333]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.15531209 0.9730327  0.89784764 0.16970959 0.81121832 0.30432665\n",
      "  0.39250113]\n",
      " [0.91869381 0.76241896 0.43706857 0.21684043 0.35084479 0.04214557\n",
      "  0.17213638]] \n",
      "\t loss = 38.55277954514996\n",
      "\n",
      "Training example #4 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]], \n",
      " \t center = [[0. 0. 0. 0. 1. 0. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.0018399   0.20147359]\n",
      " [ 0.72776396  0.75861333]\n",
      " [ 0.06767848  0.43510487]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.14742291 0.96508205 0.9023773  0.17543426 0.80495081 0.31046032\n",
      "  0.39822048]\n",
      " [0.88092481 0.7243557  0.45875407 0.24424692 0.32083947 0.07151014\n",
      "  0.1995174 ]] \n",
      "\t loss = 46.8554671159181\n",
      "\n",
      "Training example #5 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]], \n",
      " \t center = [[0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.0018399   0.20147359]\n",
      " [ 0.72776396  0.75861333]\n",
      " [ 0.06767848  0.43510487]\n",
      " [ 0.82691491  0.12930703]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[0.11939121 0.90930902 0.85213536 0.23761848 0.84769469 0.28264367\n",
      "  0.4551557 ]\n",
      " [0.87497438 0.71251647 0.44808895 0.25744709 0.32991293 0.06560536\n",
      "  0.21160333]] \n",
      "\t loss = 53.101600164554185\n",
      "\n",
      "Training example #6 \n",
      "-------------------- \n",
      "\n",
      " \t label = [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]], \n",
      " \t center = [[0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[ 0.62199038  0.30934835]\n",
      " [ 0.34471379  0.85165164]\n",
      " [-0.0018399   0.20147359]\n",
      " [ 0.72776396  0.75861333]\n",
      " [ 0.06767848  0.43510487]\n",
      " [ 0.82691491  0.12930703]\n",
      " [ 0.18352993  0.0441429 ]]\n",
      "\t W2 = [[0.11442555 0.90366712 0.84668109 0.23281731 0.86029512 0.29587872\n",
      "  0.45018321]\n",
      " [0.87251816 0.70972577 0.44539105 0.25507224 0.33614561 0.07215194\n",
      "  0.20914374]] \n",
      "\t loss = 57.022929057752144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (label, center) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    W1, W2, loss = skipgram(label, center, W1, W2, loss)\n",
    "    print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t center = {}\".format(i, label, center))\n",
    "    print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use it and check, for example, that for the context word “like” - or [0, 1, 0, 0, 0, 0, 0] - the cbow model predicts that the center word is “I”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_cbow = [1.862e-01, 2.243e-01, 1.446e-01, 9.710e-02, 1.454e-01, 8.795e-02, 1.144e-01]\n"
     ]
    }
   ],
   "source": [
    "def cbow_predict( x, W1, W2):\n",
    "        \"\"\"Predict output from input data and weights\n",
    "        :param x: input data\n",
    "        :param W1: weights from input to hidden layer\n",
    "        :param W2: weights from hidden layer to output layer\n",
    "        :return: output of neural network\n",
    "        \"\"\"\n",
    "        h = np.mean([np.matmul(W1.T, xx) for xx in x], axis=0)\n",
    "        u = np.dot(W2.T, h)\n",
    "        return softmax(u)\n",
    "x = np.array([[0, 1, 0, 0, 0, 0, 0]])\n",
    "y_pred = cbow_predict(x, W1, W2)\n",
    "print((\"prediction_cbow = [\" + 6*\"{:.3e}, \" + \"{:.3e}]\").format(*y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try a different example. We take as context words the words “I” and “playing” and we hope to see “like” as the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_cbow = [1.448e-01, 1.738e-01, 1.516e-01, 1.225e-01, 1.519e-01, 1.216e-01, 1.339e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0]])\n",
    "y_pred = cbow_predict(x, W1, W2)\n",
    "print((\"prediction_cbow = [\" + 6*\"{:.3e}, \" + \"{:.3e}]\").format(*y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the prediction is close to the expected center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is it for now. I hope you enjoyed the reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
